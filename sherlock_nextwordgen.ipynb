{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73d3b7c0",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import re\n",
    "import random\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "# config\n",
    "SEQ_LENGTH = 48\n",
    "BATCH_SIZE = 128\n",
    "EMBED_DIM = 768\n",
    "HIDDEN_DIM = 128\n",
    "NUM_LAYERS = 2\n",
    "NUM_HEADS = 4\n",
    "DROPOUT = 0.1\n",
    "LEARNING_RATE = 2e-3\n",
    "EPOCHS = 10\n",
    "PATIENCE = 2\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "MODEL_NAME = \"gpt2\"\n",
    "SEED = 5\n",
    "\n",
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "# data handling\n",
    "def download_sherlock_holmes():\n",
    "    url = \"https://www.gutenberg.org/files/1661/1661-0.txt\"\n",
    "    r = requests.get(url, timeout=15)\n",
    "    text = r.text\n",
    "    start = text.find(\"*** START OF THE PROJECT\")\n",
    "    end = text.find(\"*** END OF THE PROJECT\")\n",
    "    return text[start:end]\n",
    "\n",
    "# handling newline chars\n",
    "def preprocess_text(text):\n",
    "    return text.replace(\"\\r\\n\", \" \").replace(\"\\n\", \" \")\n",
    "\n",
    "\n",
    "#use pretrained gpt2 tokenizer\n",
    "def get_tokenizer():\n",
    "    tok = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "    if tok.pad_token is None:\n",
    "        tok.pad_token = tok.eos_token\n",
    "    return tok\n",
    "\n",
    "def load_gpt2_embeddings():\n",
    "    base = AutoModel.from_pretrained(MODEL_NAME)\n",
    "    if hasattr(base, \"wte\"):\n",
    "        return base.wte.weight.data.clone()\n",
    "    if hasattr(base, \"transformer\") and hasattr(base.transformer, \"wte\"):\n",
    "        return base.transformer.wte.weight.data.clone()\n",
    "    raise RuntimeError(\"Could not locate GPT-2 token embeddings\")\n",
    "\n",
    "class SherlockDataset(Dataset):\n",
    "    def __init__(self, ids, seq_length):\n",
    "        self.ids = ids\n",
    "        self.seq_len = seq_length\n",
    "        self.stride = seq_length\n",
    "        self.samples = []\n",
    "        for i in range(0, len(ids) - seq_length - 1, self.stride):\n",
    "            self.samples.append((i, i + seq_length + 1))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        s, e = self.samples[idx]\n",
    "        seq = self.ids[s:e]\n",
    "        return seq[:-1], seq[1:] \n",
    "\n",
    "# main architecture\n",
    "\n",
    "class SherlockLSTMAttn(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim=768, hidden_dim=768,\n",
    "                 num_layers=3, num_heads=8, dropout=0.2,\n",
    "                 pretrained_embeddings=None, freeze_embeddings=True):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        if (pretrained_embeddings is not None and \n",
    "            pretrained_embeddings.size(0) == vocab_size and \n",
    "            pretrained_embeddings.size(1) == embed_dim):\n",
    "            self.embedding.weight.data.copy_(pretrained_embeddings)\n",
    "            self.embedding.weight.requires_grad = not freeze_embeddings\n",
    "            print(f\"Loaded GPT-2 embeddings (freeze={freeze_embeddings})\")\n",
    "        else:\n",
    "            self.embedding.weight.requires_grad = True\n",
    "        \n",
    "        self.input_proj = nn.Linear(embed_dim, hidden_dim) \n",
    "        \n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=hidden_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0.0\n",
    "        )\n",
    "        \n",
    "        self.attn = nn.MultiheadAttention(\n",
    "            embed_dim=hidden_dim,\n",
    "            num_heads=num_heads,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        self.ln = nn.LayerNorm(hidden_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.output_proj = nn.Linear(hidden_dim, vocab_size, bias=False)\n",
    "    \n",
    "    def forward(self, x, hidden=None, key_padding_mask=None):\n",
    "        emb = self.embedding(x)  # (B, T, E)\n",
    "        \n",
    "        # project embeddings to hidden dim\n",
    "        h = self.input_proj(emb)  # (B, T, H)\n",
    "        \n",
    "        lstm_out, new_hidden = self.lstm(h, hidden)  # (B, T, H)\n",
    "        \n",
    "        # causal mask for training\n",
    "        T = lstm_out.size(1)\n",
    "        causal = torch.triu(torch.ones(T, T, device=x.device) * float(\"-inf\"), diagonal=1)\n",
    "        \n",
    "        # self-attention on LSTM outputs\n",
    "        attn_out, _ = self.attn(\n",
    "            query=lstm_out, key=lstm_out, value=lstm_out,\n",
    "            attn_mask=causal,\n",
    "            key_padding_mask=key_padding_mask,\n",
    "            need_weights=False\n",
    "        )\n",
    "        \n",
    "        # residual connection + LayerNorm\n",
    "        h = self.ln(lstm_out + self.dropout(attn_out))  # (B, T, H)\n",
    "        logits = self.output_proj(h)  # (B, T, vocab_size)\n",
    "        return logits, new_hidden\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def generate_text(self, tokenizer, prompt,\n",
    "                                 max_new_tokens=200, min_new_tokens=50,\n",
    "                                 temperature=0.85, top_p=0.92,\n",
    "                                 repetition_penalty=1.10,\n",
    "                                 stop_on_sentence=True,\n",
    "                                 device=DEVICE):\n",
    "        self.eval().to(device)\n",
    "        cur = tokenizer.encode(prompt, add_special_tokens=False, return_tensors=\"pt\").to(device)\n",
    "        \n",
    "        for step in range(max_new_tokens):\n",
    "            logits, _ = self(cur)\n",
    "            logits = logits[:, -1, :]\n",
    "            \n",
    "            # ban pad/bos tokens\n",
    "            for tid in [tokenizer.pad_token_id, tokenizer.bos_token_id]:\n",
    "                if tid is not None:\n",
    "                    logits[:, tid] = float(\"-inf\")\n",
    "            \n",
    "            # temperature scaling\n",
    "            logits = logits / max(temperature, 1e-8)\n",
    "            \n",
    "            # repetition penalty\n",
    "            if repetition_penalty != 1.0:\n",
    "                seen = torch.bincount(cur[0], minlength=logits.size(-1)).float().unsqueeze(0)\n",
    "                logits = logits - torch.log1p(seen) * (repetition_penalty - 1.0)\n",
    "            \n",
    "            # top-p sampling\n",
    "            probs = torch.softmax(logits, dim=-1)\n",
    "            sorted_probs, sorted_idx = torch.sort(probs, descending=True, dim=-1)\n",
    "            cumprobs = torch.cumsum(sorted_probs, dim=-1)\n",
    "            keep = cumprobs <= top_p\n",
    "            keep[..., 0] = True  \n",
    "            \n",
    "            filtered = torch.where(keep, sorted_probs, torch.zeros_like(sorted_probs))\n",
    "            filtered = filtered / filtered.sum(dim=-1, keepdim=True).clamp_min(1e-12)\n",
    "            next_sorted = torch.multinomial(filtered, num_samples=1)\n",
    "            next_token = sorted_idx.gather(-1, next_sorted)\n",
    "            \n",
    "            cur = torch.cat([cur, next_token], dim=1)\n",
    "            \n",
    "            # early stopping on sentence end\n",
    "            if stop_on_sentence and (step + 1) >= min_new_tokens:\n",
    "                text = tokenizer.decode(cur[0].tolist(), skip_special_tokens=True)\n",
    "                if re.search(r\"[.!?][\\\"')\\]]?\\s*$\", text):\n",
    "                    break\n",
    "        \n",
    "        out = tokenizer.decode(cur[0].tolist(), skip_special_tokens=True)\n",
    "        return out[len(prompt):].lstrip()\n",
    "\n",
    "# training \n",
    "def train(model, train_loader, val_loader, pad_id, epochs=EPOCHS, patience=PATIENCE, device=DEVICE):\n",
    "    model.to(device)\n",
    "    opt = optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=0.01)\n",
    "    sched = optim.lr_scheduler.ReduceLROnPlateau(opt, mode=\"min\", factor=0.5, patience=1, verbose=True)\n",
    "    crit = nn.CrossEntropyLoss(ignore_index=pad_id)\n",
    "    \n",
    "    best_loss = float(\"inf\")\n",
    "    wait = 0\n",
    "    \n",
    "    for ep in range(epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        pbar = tqdm(train_loader, desc=f\"Epoch {ep+1}/{epochs} [Train]\")\n",
    "        \n",
    "        for x, y in pbar:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            pad_mask = (x == pad_id)\n",
    "            \n",
    "            opt.zero_grad()\n",
    "            logits, _ = model(x, key_padding_mask=pad_mask)\n",
    "            loss = crit(logits.view(-1, logits.size(-1)), y.view(-1))\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "            opt.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            pbar.set_postfix(loss=f\"{loss.item():.4f}\")\n",
    "        \n",
    "        train_loss /= len(train_loader)\n",
    "        \n",
    "        # validation\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for x, y in val_loader:\n",
    "                x, y = x.to(device), y.to(device)\n",
    "                pad_mask = (x == pad_id)\n",
    "                logits, _ = model(x, key_padding_mask=pad_mask)\n",
    "                loss = crit(logits.view(-1, logits.size(-1)), y.view(-1))\n",
    "                val_loss += loss.item()\n",
    "        \n",
    "        val_loss /= len(val_loader)\n",
    "        val_ppl = math.exp(val_loss)\n",
    "        sched.step(val_loss)\n",
    "        \n",
    "        print(f\"\\nEpoch {ep+1}: Train {train_loss:.4f} | Val {val_loss:.4f} | Val PPL {val_ppl:.2f}\")\n",
    "        \n",
    "        # save best model\n",
    "        if val_loss < best_loss:\n",
    "            best_loss = val_loss\n",
    "            wait = 0\n",
    "            torch.save(model.state_dict(), \"sherlock_model.pth\")\n",
    "            print(\"  → Saved best model\")\n",
    "        else:\n",
    "            wait += 1\n",
    "            print(f\"  → No improvement ({wait}/{patience})\")\n",
    "            if wait >= patience:\n",
    "                print(\"Early stopping\")\n",
    "                break\n",
    "\n",
    "def evaluate(model, test_loader, pad_id, device=DEVICE):\n",
    "    model.to(device).eval()\n",
    "    crit = nn.CrossEntropyLoss(ignore_index=pad_id)\n",
    "    test_loss = 0.0\n",
    "    correct, total = 0, 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for x, y in tqdm(test_loader, desc=\"Testing\"):\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            pad_mask = (x == pad_id)\n",
    "            logits, _ = model(x, key_padding_mask=pad_mask)\n",
    "            loss = crit(logits.view(-1, logits.size(-1)), y.view(-1))\n",
    "            test_loss += loss.item()\n",
    "            \n",
    "            preds = logits.argmax(dim=-1)\n",
    "            mask = (y != pad_id)\n",
    "            correct += (preds[mask] == y[mask]).sum().item()\n",
    "            total += mask.sum().item()\n",
    "    \n",
    "    test_loss /= len(test_loader)\n",
    "    test_ppl = math.exp(test_loss)\n",
    "    acc = 100.0 * correct / total if total > 0 else 0.0\n",
    "    \n",
    "    print(f\"\\nTest Loss: {test_loss:.4f} | Test PPL: {test_ppl:.2f} | Top-1 Acc: {acc:.2f}%\")\n",
    "\n",
    "# main\n",
    "if __name__ == \"__main__\":\n",
    "    # load data\n",
    "    raw_text = download_sherlock_holmes()\n",
    "    text = preprocess_text(raw_text)\n",
    "    tokenizer = get_tokenizer()\n",
    "    ids = torch.tensor(tokenizer.encode(text, add_special_tokens=False), dtype=torch.long)\n",
    "    \n",
    "    # split data into train, test, val\n",
    "    n = len(ids)\n",
    "    train_end = int(0.8 * n)\n",
    "    val_end = int(0.9 * n)\n",
    "    \n",
    "    train_ids = ids[:train_end]\n",
    "    val_ids = ids[train_end:val_end]\n",
    "    test_ids = ids[val_end:]\n",
    "    \n",
    "    # create datasets and loaders\n",
    "    train_ds = SherlockDataset(train_ids, SEQ_LENGTH)\n",
    "    val_ds = SherlockDataset(val_ids, SEQ_LENGTH)\n",
    "    test_ds = SherlockDataset(test_ids, SEQ_LENGTH)\n",
    "    \n",
    "    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, pin_memory=True)\n",
    "    val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, pin_memory=True)\n",
    "    test_loader = DataLoader(test_ds, batch_size=BATCH_SIZE, pin_memory=True)\n",
    "    \n",
    "    # build model\n",
    "    gpt2_embeddings = load_gpt2_embeddings()\n",
    "    model = SherlockLSTMAttn(\n",
    "        vocab_size=tokenizer.vocab_size,\n",
    "        embed_dim=EMBED_DIM,\n",
    "        hidden_dim=HIDDEN_DIM,\n",
    "        num_layers=NUM_LAYERS,\n",
    "        num_heads=NUM_HEADS,\n",
    "        dropout=DROPOUT,\n",
    "        pretrained_embeddings=gpt2_embeddings,\n",
    "        freeze_embeddings=True\n",
    "    )\n",
    "    \n",
    "    print(f\"Vocab: {tokenizer.vocab_size} | Params: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "    \n",
    "    # train\n",
    "    pad_id = tokenizer.pad_token_id\n",
    "    train(model, train_loader, val_loader, pad_id, epochs=EPOCHS, patience=PATIENCE, device=DEVICE)\n",
    "    \n",
    "    # evaluate\n",
    "    model.load_state_dict(torch.load(\"sherlock_model.pth\", map_location=DEVICE))\n",
    "    evaluate(model, test_loader, pad_id, device=DEVICE)\n",
    "    \n",
    "    # generation\n",
    "\n",
    "    prompts = [\n",
    "      \"“Well, Holmes,” said I,\",\n",
    "      \"“My dear Watson,” Holmes replied,\",\n",
    "      \"Holmes asked, “And what do you make of it?”\",\n",
    "      \"“Come in, Inspector,” said Holmes,\",\n",
    "    ]\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Generation with temperature=0.85 | top_p=0.92\")\n",
    "    print('='*80)\n",
    "    \n",
    "    for prompt in prompts:\n",
    "        output = model.generate_text(\n",
    "            tokenizer, prompt,\n",
    "            max_new_tokens=160, min_new_tokens=40,\n",
    "            temperature=0.85, top_p=0.92,\n",
    "            repetition_penalty=1.12, stop_on_sentence=True\n",
    "        )\n",
    "        print(f\"\\n▶ Prompt: {prompt}\")\n",
    "        print(f\"   {prompt} {output}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
